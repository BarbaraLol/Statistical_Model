---
title: "Report_G_Group"
author: "Alberto Russo, Andrea Esposito, Barbara Loletti, Gabriel Masella, Ziyang Fu"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
# Load required libraries
library(randomForest)
library(caret)
library(Metrics)
library(gridExtra)
library(dplyr)
library(ggplot2)
library(car)
library(rpart)
library(rpart.plot)

knitr::opts_chunk$set(echo = TRUE)
```

```{r functions, echo=TRUE, include=FALSE}
# Define likelihood_ratio_test function
likelihood_ratio_test <- function(full_model, null_model) {
  LR_statistic <- -2 * (logLik(null_model) - logLik(full_model))
  df <- df.null - df.full
  p_val <- pchisq(LR_statistic, df, lower.tail = FALSE)
  return(c(LR_statistic, p_val))
}

# Set the number of CPU cores to use
num_cores <- parallel::detectCores()
cat("Number of CPU cores:", num_cores, "\n")
```

```{r data, echo=TRUE}
# Define the current working directory
cwd <- getwd()

# Load the data
data <- read.csv(file.path(cwd, 'health_data.csv'))

# Display the first few rows of the data
head(data)
```

```{r data_cleaning, echo=TRUE}
# Create a working copy to avoid modifying the original data
df <- data

# Define the list of categorical variables
categorical <- c('gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio')

# Replace infinite values with NA in the entire data frame
df <- as.data.frame(lapply(df, function(x) replace(x, is.infinite(x), NA)))

# Remove NaNs, although according to the data description there are no NaNs
df <- na.omit(df)

# Drop the id column and -1
df <- df[, !(names(df) %in% c('id', 'X'))]

# Convert age in days
df$age <- df$age / 365.25

# Filter for impossible values for height, weight, ap_hi, ap_lo
df <- df[df$height > 130 & df$height < 220, ]
df <- df[df$weight > 30 & df$weight < 200, ]
df <- df[df$ap_hi > 60 & df$ap_hi < 240, ]
df <- df[df$ap_lo > 40 & df$ap_lo < 200, ]
df <- df[df$age < 100, ]

# Add a new column for the BMI
df$BMI <- df$weight / (df$height / 100)^2

# Filter for reasonable values for BMI
df <- df[df$BMI > 6 & df$BMI < 70, ]

# Convert all categorical variables to factors
df[categorical] <- lapply(df[categorical], as.factor)

# Display the first few rows of the cleaned data
head(df)
```

```{r dataset_analysis_summary, echo=TRUE}
# Summary statistics
summary(df)
```

```{r dataset_analysis, echo=TRUE}
# Create a copy of the cleaned dataset to use for EDA
df_eda <- df

#########################
## Numerical Variables ##
#########################

# Create the layout for the plots
par(mfrow=c(2, 2), mar=c(5, 4, 4, 2) + 0.1, oma=c(0, 0, 2, 0))

# Create histograms for the numerical variables
hist(df_eda$age, breaks=seq(min(df_eda$age), max(df_eda$age) + 1.5, by=1.5), col='lightblue', main='Age Distribution', xlab='Age')

hist(df_eda$height, breaks=seq(min(df_eda$height), max(df_eda$height) + 2, by=2), col='lightblue', main='Height Distribution', xlab='Height')

hist(df_eda$weight, breaks=seq(min(df_eda$weight), max(df_eda$weight) + 3, by=3), col='lightblue', main='Weight Distribution', xlab='Weight')

hist(df_eda$BMI, breaks=seq(min(df_eda$BMI), max(df_eda$BMI) + 1, by=1), col='lightblue', main='BMI Distribution', xlab='BMI')

# Add a title to the plots
mtext("Numeric Variables", outer=TRUE, cex=1.5)

###########################
## Categorical Variables ##
###########################

# Create the layout for the plots
par(mfrow=c(2, 3), mar=c(5, 4, 4, 2) + 0.1, oma=c(0, 0, 2, 0))

# Create bar plots for the categorical variables
barplot(table(df_eda$gender), col='lightblue', main='Gender Distribution', xlab='Gender')

barplot(table(df_eda$cholesterol), col='lightblue', main='Cholesterol Distribution', xlab='Cholesterol')

barplot(table(df_eda$gluc), col='lightblue', main='Glucose Distribution', xlab='Glucose')

barplot(table(df_eda$smoke), col='lightblue', main='Smoking Distribution', xlab='Smoking')

barplot(table(df_eda$alco), col='lightblue', main='Alcohol Consumption Distribution', xlab='Alcohol Consumption')

barplot(table(df_eda$active), col='lightblue', main='Physical Activity Distribution', xlab='Physical Activity')

# Add a title to the plots
mtext("Categorical Variables", outer=TRUE, cex=1.5)
```

```{r EDA, echo=TRUE}

########################
## CORRELATION MATRIX ##
########################

# Assuming df_eda is your dataframe
# Filter out non-numeric columns
df_numeric <- df_eda %>% select_if(is.numeric)

# Calculate the correlation matrix on the numeric dataframe
cor_matrix <- cor(df_numeric) %>% as.matrix()

# Create the heatmap
heatmap(cor_matrix,
        Rowv = NA, Colv = NA,
        col = colorRampPalette(c("blue", "white", "red"))(30),
        main = 'Correlation Matrix',
        margins = c(5,5),
        cexRow = 1, cexCol = 1,
        symm = TRUE,
        revC = TRUE)

#######################################################
## DISTRIBUTION OF DATA ACCORDING TO TARGET VARIABLE ##
#######################################################

## Numerical ##

# Create the layout for the plots
par(mfrow=c(3, 3), mar=c(5, 4, 4, 2) + 0.1, oma=c(0, 0, 2, 0))

# Create histograms for the numerical variables
hist(df_eda$age[df_eda$cardio == 0], col='lightblue', main='Age Distribution \n by Cardiovascular Disease (No)', xlab='Age', breaks=seq(min(df_eda$age), max(df_eda$age) + 1.5, by=1.5), xlim=c(min(df_eda$age), max(df_eda$age) + 1.5))

hist(df_eda$age[df_eda$cardio == 1], col='lightcoral', main='Age Distribution \n by Cardiovascular Disease (Yes)', xlab='Age', breaks=seq(min(df_eda$age), max(df_eda$age) + 1.5, by=1.5), xlim=c(min(df_eda$age), max(df_eda$age) + 1.5))

hist(df_eda$height[df_eda$cardio == 0], col='lightblue', main='Height Distribution \n by Cardiovascular Disease (No)', xlab='Height', breaks=seq(min(df_eda$height), max(df_eda$height) + 2, by=2), xlim=c(min(df_eda$height), max(df_eda$height) + 2))

hist(df_eda$height[df_eda$cardio == 1], col='lightcoral', main='Height Distribution \n by Cardiovascular Disease (Yes)', xlab='Height', breaks=seq(min(df_eda$height), max(df_eda$height) + 2, by=2), xlim=c(min(df_eda$height), max(df_eda$height) + 2))

hist(df_eda$weight[df_eda$cardio == 0], col='lightblue', main='Weight Distribution \n by Cardiovascular Disease (No)', xlab='Weight', breaks=seq(min(df_eda$weight), max(df_eda$weight) + 3, by=3), xlim=c(min(df_eda$weight), max(df_eda$weight) + 3))

hist(df_eda$weight[df_eda$cardio == 1], col='lightcoral', main='Weight Distribution \n by Cardiovascular Disease (Yes)', xlab='Weight', breaks=seq(min(df_eda$weight), max(df_eda$weight) + 3, by=3), xlim=c(min(df_eda$weight), max(df_eda$weight) + 3))

hist(df_eda$BMI[df_eda$cardio == 0], col='lightblue', main='BMI Distribution \n by Cardiovascular Disease (No)', xlab='BMI', breaks=seq(min(df_eda$BMI), max(df_eda$BMI) + 1, by=1), xlim=c(min(df_eda$BMI), max(df_eda$BMI) + 1))

hist(df_eda$BMI[df_eda$cardio == 1], col='lightcoral', main='BMI Distribution \n by Cardiovascular Disease (Yes)', xlab='BMI', breaks=seq(min(df_eda$BMI), max(df_eda$BMI) + 1, by=1), xlim=c(min(df_eda$BMI), max(df_eda$BMI) + 1))

# Add a title to the plots
mtext("Numerical Variables", outer=TRUE, cex=1.5)

```

```{r VIF, echo=TRUE}
df_vif <- df

# Drop the height column
df_vif <- df_vif %>% select(-height)

no_height <- glm(cardio ~ ., data = df_vif, family = binomial(link = "logit"))

summary(no_height)

vif(no_height)

```

Do not use the height variable, because it is higly correlated to the BMI and to the weight, with a small correlation with age, while having a low correlation with the target variable.\
It's basically just there to make the esitmation of the variance of the other covariates worse.

```{r logistic_reg, echo=TRUE}
# Drop the height column and make a copy
df_logi <- df %>% select(-height)

# Fit the logistic regression model
logi_model <- glm(cardio ~ ., data = df_logi, family = binomial(link = "logit"))

# Summary of the model
summary(logi_model)

# Compare the model with the null model
anova(logi_model, test = "Chisq")
```

The full model is significantly better than the null model, but some of the variables are not statistically significant, except for glucose, for which the level 1 is not significant while the level 2 is, but it's negatively correlated with the target variable. This looks like a selection bias problem: it's likely that people with very high glucose levels have diabetes, so it's possible that it's a spurious relationship.

```{r logistic_reg_2, echo=TRUE}
# Drop the gender1 and the gluc1 columns and make a copy
df_logi_2 <- df_logi %>% select(-gender, -gluc)

# Fit the logistic regression model
logi_model_2 <- glm(cardio ~ ., data = df_logi_2, family = binomial(link = "logit"))

# Summary of the model
summary(logi_model_2)

# Compare the model with the null model
anova(logi_model_2, test = "Chisq")

# Compare the two models
anova(logi_model, logi_model_2, test = "Chisq")
```

The second model is significantly better than the null model, and it is also significantly better than the first model.

## Tree
Parametri da tunare>


```{r TREE, echo=TRUE}

# Create a copy without the excluded variables
df_tree<-df %>% select(-gender, -gluc, -height)

# Perform a train-test split (80/20)
set.seed(42) # For reproducibility

# Set the parameters for the decision tree
cp = c(0.01, 0.001, 0.0001)
minsplit = c(10, 30, 100)
minbucket = c(10, 30, 100)
maxdepth = c(10, 20, 30)

trainIndex <- createDataPartition(df_tree$cardio, p = 0.8, list = FALSE)

df_train <- df_tree[trainIndex, ]

df_test <- df_tree[-trainIndex, ]

# Create a decision tree classifier with parameters adjusted for more branches
dt <- rpart(formula = cardio ~ ., 
            data = df_train, 
            method = "class", 
            control = rpart.control(minsplit = 10,    # Lowered to allow more splits
                                    minbucket = 100,  # Lowered to allow smaller leaf nodes
                                    maxdepth = 10,    # Increased to allow deeper trees
                                    ))

# If you want to visualize the tree
rpart.plot(dt)

```

```{r CV, echo=TRUE}
# Set the seed for reproducibility
set.seed(42)

# Create a grid search for the decision tree
tuneGrid <- expand.grid(cp = c(0.01, 0.001, 0.0001),
                        minsplit = c(10, 30, 100),
                        minbucket = c(10, 30, 100),
                        maxdepth = c(10, 20, 30))

# Perform a 5-fold cross-validation
cv <- train(form = cardio ~ ., 
            data = df_tree, 
            method = "rpart", 
            trControl = trainControl(method = "cv", number = 5)
)

# Print the results
print(cv)
```


```{r echo=TRUE}
# Create a copy of the cleaned dataset to use for feature importance
df_forest <- df
class(df)
# Assuming 'column_to_drop' is the name of the column you want to drop
column_to_drop <- "X"

# Drop the specified column
df_forest <- df_forest[, !names(df_forest) %in% column_to_drop]
summary(df_forest)


# Perform a train-test split (80/20)
trainIndex <- createDataPartition(df_forest$cardio, p = 0.8, list = FALSE)
df_train <- df_forest[trainIndex, ]
df_test <- df_forest[-trainIndex, ]

# Divide the data into features and target for training
X_train <- df_train[, !(names(df_train) %in% c('cardio'))]
y_train <- df_train$cardio

# Divide the data into features and target for testing
X_test <- df_test[, !(names(df_test) %in% c('cardio'))]
y_test <- df_test$cardio

# Capture the start time
start_time <- Sys.time()
# Create a random forest classifier
rf <- randomForest(x = X_train, y = y_train, ntree = 500, importance = TRUE, seed = 42)
# Capture the finish time
finish_time <- Sys.time()
# Calculate and print the duration
duration <- finish_time - start_time
print(paste("Duration:", duration))
print(rf)

# Get the feature importances
feature_importances <- importance(rf)
str(feature_importances)
print(feature_importances)
```


