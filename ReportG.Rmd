---
title: "Report_G_Group"
author: "Alberto Russo, Andrea Esposito, Barbara Loletti, Gabriel Masella, Ziyang Fu"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
# Load required libraries
library(randomForest)
library(caret)
library(Metrics)
library(gridExtra)
library(dplyr)
library(ggplot2)
library(car)
library(rpart)
library(rpart.plot)

knitr::opts_chunk$set(echo = TRUE)
# knitr::include_graphics("./heat_matrix.png")
```

```{r functions, include=FALSE, echo=FALSE, include=FALSE}
# Define likelihood_ratio_test function
likelihood_ratio_test <- function(full_model, null_model) {
  LR_statistic <- -2 * (logLik(null_model) - logLik(full_model))
  df <- df.null - df.full
  p_val <- pchisq(LR_statistic, df, lower.tail = FALSE)
  return(c(LR_statistic, p_val))
}

```
# Aim of the project
The aim of the current project is to build a model to predict cardiac disease using data obtained from medical and demographical information of the patients.  

The first goal is to identify the main predictors to use in medical analysis, while the second one is to build a model to directly try to predict the status of the patient, in order to intervene early with preventive care.

# Import the data

```{r data, echo=TRUE}
# Define the current working directory
cwd <- getwd()

# Load the data
data <- read.csv(file.path(cwd, 'health_data.csv'))

# Display the first few rows of the data
head(data)
```

The data includes columns for the following variables:

-   id: a unique identifier for each patient;

-   an unnamed column as an index;

-   age: the age of the patient in days;

-   gender: a categorical feature for the sex of the patient, with 0 for "female" and 1 for "male";

-   height: height of patients in cm;

-   weight: the weight of the patients in kg;

-   ap_hi: systolic blood pressure, in mmHg;

-   ap_lo: diastolic blood pressure, in mmHg;

-   cholesterol: a categorical feature that codifies as 0 a normal level of cholesterol, 1 an above average level of cholesterol and with 2 a well above average level of cholesterol;

-   gluc: a categorical feature that codifies as 0 a normal level of glucose, 1 an above average level of glucose and with 2 a well above average level of glucose;

-   smoke: codifies whether the patient is a smoker or not;

-   alco: codifies whether the patient is an habitual alcohol consumer;

-   active: codifies whether the patient is physically active or not;

-   cardio: the target variable, codifies whether the patient has been diagnosed with a cardiac disease.

It's necessary to filter these columns by dropping the first two columns which are useless, converting the age from days into years, filtering for NaNs, Infs, eliminate absurd values (there are, for instance, negative pressure values, or values well above what's physically possible, and some that are obvious input mistakes, but in a complicated way that would require edit by hand of each "wrong" value). A BMI column is added, since we expect height to be of little importance and the weight alone is insufficient to determine the body type of a person.

```{r data_cleaning, echo=TRUE}
# Create a working copy to avoid modifying the original data
df <- data

# Define the list of categorical variables
categorical <- c('gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio')

# Replace infinite values with NA in the entire data frame
df <- as.data.frame(lapply(df, function(x) replace(x, is.infinite(x), NA)))

# Remove NaNs, although according to the data description there are no NaNs
df <- na.omit(df)

# Drop the id column and -1
df <- df[, !(names(df) %in% c('id', 'X'))]

# Convert age in days
df$age <- df$age / 365.25

# Filter for impossible values for height, weight, ap_hi, ap_lo
df <- df[df$height > 130 & df$height < 220, ]
df <- df[df$weight > 30 & df$weight < 200, ]
df <- df[df$ap_hi > 60 & df$ap_hi < 240, ]
df <- df[df$ap_lo > 40 & df$ap_lo < 200, ]
df <- df[df$age < 100, ]

# Add a new column for the BMI
df$BMI <- df$weight / (df$height / 100)^2

# Filter for reasonable values for BMI
df <- df[df$BMI > 6 & df$BMI < 70, ]

# Convert all categorical variables to factors
df[categorical] <- lapply(df[categorical], as.factor)

# Display the first few rows of the cleaned data
head(df)
```

# EDA
Here's the summary statistics for the cleaned dataset:
```{r dataset_analysis_summary, echo=TRUE}
# Summary statistics
summary(df)
```

The cleaned dataset contains 68,000 observations and 13 variables. The target variable is "cardio", which is a binary variable that indicates whether a patient has been diagnosed with a cardiac disease or not. The other variables include 6 numerical variables and 6 categorical variables, as listed above.  
Let's now proceed with the exploratory data analysis (EDA) to better understand the distribution of the variables and their relationships with the target variable. The R code is for illustrative purposes only, the plots have been obtained in Python and are displayed below.

```{r dataset_analysis_plots, echo=FALSE, eval=TRUE}
knitr::include_graphics("./numerical_variables.png")
knitr::include_graphics("./categorical_variables.png")

```

```{r dataset_analysis, eval=FALSE, echo=TRUE}
# Create a copy of the cleaned dataset to use for EDA
df <- df

#########################
## Numerical Variables ##
#########################

# Create the layout for the plots
par(mfrow=c(2, 2), mar=c(5, 4, 4, 2) + 0.1, oma=c(0, 0, 2, 0))

# Create histograms for the numerical variables
hist(df$age, breaks=seq(min(df$age), max(df$age) + 1.5, by=1.5), col='lightblue', main='Age Distribution', xlab='Age')

hist(df$height, breaks=seq(min(df$height), max(df$height) + 2, by=2), col='lightblue', main='Height Distribution', xlab='Height')

hist(df$weight, breaks=seq(min(df$weight), max(df$weight) + 3, by=3), col='lightblue', main='Weight Distribution', xlab='Weight')

hist(df$BMI, breaks=seq(min(df$BMI), max(df$BMI) + 1, by=1), col='lightblue', main='BMI Distribution', xlab='BMI')

# Add a title to the plots
mtext("Numeric Variables", outer=TRUE, cex=1.5)

###########################
## Categorical Variables ##
###########################

# Create the layout for the plots
par(mfrow=c(2, 3), mar=c(5, 4, 4, 2) + 0.1, oma=c(0, 0, 2, 0))

# Create bar plots for the categorical variables
barplot(table(df$gender), col='lightblue', main='Gender Distribution', xlab='Gender')

barplot(table(df$cholesterol), col='lightblue', main='Cholesterol Distribution', xlab='Cholesterol')

barplot(table(df$gluc), col='lightblue', main='Glucose Distribution', xlab='Glucose')

barplot(table(df$smoke), col='lightblue', main='Smoking Distribution', xlab='Smoking')

barplot(table(df$alco), col='lightblue', main='Alcohol Consumption Distribution', xlab='Alcohol Consumption')

barplot(table(df$active), col='lightblue', main='Physical Activity Distribution', xlab='Physical Activity')

# Add a title to the plots
mtext("Categorical Variables", outer=TRUE, cex=1.5)
```

Now the distributions by target variable:

```{r dataset_analysis_target, eval=TRUE, echo=TRUE}

knitr::include_graphics("./numerical_distributions.png")
knitr::include_graphics("./categorical_distributions.png")
```

According to the distributions, we can see that the difference between the distributions of the height is entirely identical between the two classes, while a difference exists for the other numerical variables. We'll eliminate the height variable from the analysis.  
For the categorical variables, we can see that the distributions are fairly different for some of the variables, but it's hard to evaluate if the observed differences are given by an actual relationship or just by chance. We'll proceed with the analysis to better understand the relationships between the variables and the target variable, after plotting the correlation matrix.

```{r corr_matrix, eval=TRUE, echo=TRUE}

knitr::include_graphics("./heat_matrix.png")

```

```{r EDA, eval=FALSE, echo=TRUE}

########################
## CORRELATION MATRIX ##
########################

# Assuming df is your dataframe
# Filter out non-numeric columns
df_numeric <- df %>% select_if(is.numeric)

# Calculate the correlation matrix on the numeric dataframe
cor_matrix <- cor(df_numeric) %>% as.matrix()

# Create the heatmap
heatmap(cor_matrix,
        Rowv = NA, Colv = NA,
        col = colorRampPalette(c("blue", "white", "red"))(30),
        main = 'Correlation Matrix',
        margins = c(5,5),
        cexRow = 1, cexCol = 1,
        symm = TRUE,
        revC = TRUE)

#######################################################
## DISTRIBUTION OF DATA ACCORDING TO TARGET VARIABLE ##
#######################################################

## Numerical ##

# Create the layout for the plots
par(mfrow=c(3, 3), mar=c(5, 4, 4, 2) + 0.1, oma=c(0, 0, 2, 0))

# Create histograms for the numerical variables
hist(df$age[df$cardio == 0], col='lightblue', main='Age Distribution \n by Cardiovascular Disease (No)', xlab='Age', breaks=seq(min(df$age), max(df$age) + 1.5, by=1.5), xlim=c(min(df$age), max(df$age) + 1.5))

hist(df$age[df$cardio == 1], col='lightcoral', main='Age Distribution \n by Cardiovascular Disease (Yes)', xlab='Age', breaks=seq(min(df$age), max(df$age) + 1.5, by=1.5), xlim=c(min(df$age), max(df$age) + 1.5))

hist(df$height[df$cardio == 0], col='lightblue', main='Height Distribution \n by Cardiovascular Disease (No)', xlab='Height', breaks=seq(min(df$height), max(df$height) + 2, by=2), xlim=c(min(df$height), max(df$height) + 2))

hist(df$height[df$cardio == 1], col='lightcoral', main='Height Distribution \n by Cardiovascular Disease (Yes)', xlab='Height', breaks=seq(min(df$height), max(df$height) + 2, by=2), xlim=c(min(df$height), max(df$height) + 2))

hist(df$weight[df$cardio == 0], col='lightblue', main='Weight Distribution \n by Cardiovascular Disease (No)', xlab='Weight', breaks=seq(min(df$weight), max(df$weight) + 3, by=3), xlim=c(min(df$weight), max(df$weight) + 3))

hist(df$weight[df$cardio == 1], col='lightcoral', main='Weight Distribution \n by Cardiovascular Disease (Yes)', xlab='Weight', breaks=seq(min(df$weight), max(df$weight) + 3, by=3), xlim=c(min(df$weight), max(df$weight) + 3))

hist(df$BMI[df$cardio == 0], col='lightblue', main='BMI Distribution \n by Cardiovascular Disease (No)', xlab='BMI', breaks=seq(min(df$BMI), max(df$BMI) + 1, by=1), xlim=c(min(df$BMI), max(df$BMI) + 1))

hist(df$BMI[df$cardio == 1], col='lightcoral', main='BMI Distribution \n by Cardiovascular Disease (Yes)', xlab='BMI', breaks=seq(min(df$BMI), max(df$BMI) + 1, by=1), xlim=c(min(df$BMI), max(df$BMI) + 1))

# Add a title to the plots
mtext("Numerical Variables", outer=TRUE, cex=1.5)

```

Let's now checkout the variance inflation factor (VIF) to check for multicollinearity between the numerical variables.

```{r VIF, echo=TRUE}

df_vif <- df

# Drop the height column
df_vif <- df_vif %>% select(-height)

no_height <- glm(cardio ~ ., data = df_vif, family = binomial(link = "logit"))

summary(no_height)

vif(no_height)

```

It seems that the collinearity is reasonably low, so we can proceed with the analysis.

# Logistic Regression

## Full model

```{r logistic_reg, echo=TRUE}
# Drop the height column and make a copy
df_logi <- df %>% select(-height)

# Fit the logistic regression model
logi_model <- glm(cardio ~ ., data = df_logi, family = binomial(link = "logit"))

# Summary of the model
summary(logi_model)
```

The full model is significantly better than the null model, since the null deviance is greater than the residual deviance.  
Some of the variables are not statistically significant, except for glucose, for which the level 1 is not significant while the level 2 is. However, it's negatively correlated with the target variable, contraddicting current medical knowledge.  
We therefore hypothesize that glucose has a spurious relationship with the target variable, so we will thus drop it from the model. 

## Nested model

We now fit a logistic model dropping the previously discussed explanatory variables:

```{r logistic_reg_2, echo=TRUE}
# Drop the gender1 and the gluc1 columns and make a copy
df_logi_2 <- df_logi %>% select(-gender, -gluc)

# Fit the logistic regression model
logi_model_2 <- glm(cardio ~ ., data = df_logi_2, family = binomial(link = "logit"))

# Summary of the model
summary(logi_model_2)
```

This model is significantly different than the null one, while every variable is statistically significant.

## Comparison between the full and nested models

```{r comaprison, echo=TRUE, eval=TRUE}
# Compare the two models
anova(logi_model, logi_model_2, test = "Chisq")
```

Given the similarity of the deviance values, the lower complexity and the considerations about the gluc variable, we select the nested model.

## Tree

### Pre-pruning

We opted for pre-pruning to prevent overfitting. This method is also known as early stopping. 
It consists in setting stopping criteria as parameter values, allowing the tree to grow until the it meets any of the pre-pruning criteria. 

The tested paramaters are the following: 

* _cp_ or _complexity parameter_, refers to the cost of adding additional variables as splitting nodes

* _min\_bucket_ refers to the minimum number of samples in a terminal (leaf) node

* _max\_depth_ refers to the maximum tree height after which the tree stops growing 

* _min\_split_ refers to the minimum number of samples in a node to attempt a split


```{r TREE, echo=TRUE}

# Create a copy without the excluded variables
df_tree<-df %>% select(-gender, -gluc, -height)

# Perform a train-test split (80/20)
set.seed(42) # For reproducibility

# Set the parameters for the decision tree
cp = c(0.01, 0.001, 0.0001)
minsplit = c(10, 30, 100)
minbucket = c(10, 30, 100)
maxdepth = c(10, 20, 30)

trainIndex <- createDataPartition(df_tree$cardio, p = 0.8, list = FALSE)

df_train <- df_tree[trainIndex, ]

df_test <- df_tree[-trainIndex, ]

# Create a decision tree classifier with parameters adjusted for more branches
dt <- rpart(formula = cardio ~ ., 
            data = df_train, 
            method = "class", 
            control = rpart.control(minsplit = 10,   # Lowered to allow more splits
                                    minbucket = 10,  # Lowered to allow smaller leaf nodes
                                    maxdepth = 10    # Increased to allow deeper trees
                                    ))

# If you want to visualize the tree
rpart.plot(dt)

```

The tree shows that the most important predictor by far is the ap_hi, for which a child is directly a terminal node. The second most important predictor seems to be age, together with very high cholesterol levels.

Through cross-validation we found the parameters combinations that resulted in the best accuracy performance: 

```{r CV, eval=FALSE, echo=TRUE}

cps = c(0.01, 0.001, 0.0001)
min_buccket = c(10, 30, 100)
max_depth = c(10, 20, 30)
min_split = c(10, 30, 100)
hps = expand.grid(cps=cps, min_buccket=min_buccket, max_depth=max_depth, min_split=min_split, acc=c(-1))
k = 10

set.seed(123)  # Set seed for reproducibility
folds <- createFolds(df_tree$cardio, k = k)

for (i in 1:nrow(hps)) {
  # Initialize vector to store fold errors
  fold_errors <- numeric(10)
  
  for (fold in 1:k) {
    # Split data into training and validation sets
    train_indices <- unlist(folds[-fold])
    validation_indices <- unlist(folds[fold])
    print(hps[i, 3])
    model = rpart(formula = cardio ~ ., 
            data = df_tree[train_indices,], 
            method = "class", 
            control = rpart.control(maxdepth = hps[i, 3], 
                                    minsplit = hps[i, 4],
                                    minbucket = hps[i, 2], 
                                    cp = hps[i, 1]))
    test = df_tree[validation_indices, ]
    y_pred = predict(model, test, type="class")
    acc = mean(y_pred == test$cardio)
    hps[i, 5] = acc
  }
}

```

## Random Forest

```{r FOREST, eval=TRUE, echo=TRUE}
# Create a copy of the cleaned dataset to use for feature importance
df_forest <- df_tree

# Perform a train-test split (80/20)
trainIndex <- createDataPartition(df_forest$cardio, p = 0.8, list = FALSE)

df_train <- df_forest[trainIndex, ]
df_test <- df_forest[-trainIndex, ]

# Divide the data into features and target for training
X_train <- df_train[, !(names(df_train) %in% c('cardio'))]
y_train <- df_train$cardio

# Divide the data into features and target for testing
X_test <- df_test[, !(names(df_test) %in% c('cardio'))]
y_test <- df_test$cardio

# Create a random forest classifier
rf <- randomForest(x = X_train, y = y_train, ntree = 500, importance = TRUE, seed = 42)

# Make predictions
y_pred <- predict(rf, X_test)

# Print the confusion matrix
confusionMatrix(y_pred, y_test)

# Get the feature importances
feature_importances <- importance(rf)
print(feature_importances)
```

The accuracy obtained is 0.73, with a specificity 0.70, a sensitivity of 0.76. These result are pretty disappointing, considering that the performance is about the same as the simple pre-pruned tree.  

We can use the Random Forest Classifier to obtain a ranking of the feature importance:

# Logistic with the most important variable

Let's try using just the most important variable for the logistic regression and see how it compares to the previously selected model:
```{r most_important, echo=TRUE, eval=TRUE}
df_logi_3 <- df_tree %>% select(cardio, ap_hi)

# Fit the logistic regression model
logi_model_3 <- glm(cardio ~ ., data = df_logi_3, family = binomial(link = "logit"))

# Summary of the model
summary(logi_model_3)

```

As expected from the correlation matrix, the most important feature is the ap_hi one.

## Comparison with the previously selected model

```{r most_important_comparison, echo=TRUE, eval=TRUE}
anova(logi_model_2, logi_model_3)
```

The performance is worse, but the complex model is still much closer to the one variable model than one would expect. Most of the gained precision from the null model is given by the ap_hi feature, confirming the findings of the Random Forest Classifier.

# Results

The results are that the most important predictors are the systolic blood pressure, age and very high levels of cholesterol.  

We have identified a potential bias in glucose levels data, which might be linked to other pathologies.  

The best statistical model is the reduced logistic regression model, because it presents a very similar deviance to the full model but with less complexity.  

The best model for prediction, according to accuracy, complexity metrics and efficiency, is the pre-pruned tree, since it presents the same performance as the more complex and more computationally intensive random forest.